



# 概述

统计学习基本分类

| 类型       | 数据需求                  | 学习方式                 | 典型应用           |
| ---------- | ------------------------- | ------------------------ | ------------------ |
| 监督学习   | 大量有标签数据            | 学输入 → 输出映射        | 图像分类、房价预测 |
| 无监督学习 | 无标签数据                | 找规律和结构             | 聚类、降维         |
| 半监督学习 | 少量有标签 + 大量无标签   | 结合监督和无监督         | 医学影像、语音识别 |
| 强化学习   | 与环境交互，奖励信号      | 试错学习，最大化长期回报 | 游戏 AI、机器人    |
| 主动学习   | 少量人工标注 + 大量未标注 | 模型主动挑选难样本标注   | 智能标注、医疗     |

按模型分类

| 模型类型     | 特点（通俗版）                                 | 举例                                 |
| ------------ | ---------------------------------------------- | ------------------------------------ |
| 概率模型     | 输出带“概率”，告诉你事情发生的可能性           | 逻辑回归、朴素贝叶斯、隐马尔可夫模型 |
| 非概率模型   | 只给结果，不告诉你可能性有多大                 | KNN、SVM、普通线性回归               |
| 线性模型     | 输入和参数“直线组合”，关系简单好理解           | 线性回归、逻辑回归、线性 SVM         |
| 非线性模型   | 输入和输出关系复杂，不是直线，可以拟合弯曲曲线 | 决策树、随机森林、神经网络           |
| 参数化模型   | 模型用固定数量的参数表示，数据多也不变         | 线性回归、逻辑回归、神经网络         |
| 非参数化模型 | 模型会根据数据量“自动长大”，数据越多模型越复杂 | KNN、决策树、高斯过程                |



## 模型概述

| 模型       | 实际应用             | 现状                  | 学习意义                       |
| ---------- | -------------------- | --------------------- | ------------------------------ |
| 感知机     | 简单二分类           | 基础/历史             | 理解线性分类和神经网络基础     |
| KNN        | 图像、推荐           | 小数据集可用          | 理解距离度量、局部学习         |
| 朴素贝叶斯 | 文本分类、垃圾邮件   | 仍广泛                | 概率模型基础、快速简单         |
| 决策树     | 信贷、医疗、客户分类 | 常用，基础模型        | 理解树模型、过拟合、特征重要性 |
| 逻辑回归   | 信用评分、医学预测   | 常用，小数据高解释性  | 概率分类基础、解释性强         |
| 最大熵     | NLP、词性标注        | NLP仍用，深度学习替代 | 多类别概率建模基础             |



# 感知机

> 一条直线或者平面，把样本分为正类/负类

给定输入特征，把样本分到 **正类（+1）** 或 **负类（-1）**

f(x)=w⋅x+b

f(x)>0: 正

f(x)<0: 负



学习目标：找到一个能把样本正确分类的平面，中间一定会有误差，但是要把误差最小化

算法：更新参数 w b，使损失函数最小化

使用梯度下降，极小化误分类点的梯度下降





# K临近算法 KNN

> 有一些标注好的数据，新来一个点获取周围 K 个点的分类标签，根据投票的方式决定这个点是哪个值

要预测时，直接看新样本周围的 k 个邻居



k的影响：

K小：使用较小邻域中的数据，如果恰巧是噪声会预测错





# 朴素贝叶斯 Naive Bayes

> 一个新的点，计算它属于每个类别的概率，然后选概率最大的类别作为预测结果

根据过往的条件概率，来做分类

朴素：假设每个特征之间是相互独立的



**分辨垃圾邮件问题**

- 找邮件的特征：免费、中奖、！！！等
- 这些是垃圾邮件的特征
- 邮件中出现这些关键字就更可能是垃圾邮件

根据过去的邮件经验，看每个特征，综合判断新邮件属于哪类







# 决策树模型

> 每次问一个问题，根据回答走向下一步，直到猜到类别

if-then 思想

需要选取最具有分类能力的特征进行分类



ID3算法

用信息增益选择划分特征。

**信息增益** = 划分前数据的不确定性 - 划分后子集的不确定性

特征能让数据纯度提升最多，就选它。

缺点：偏向取值多的特征



C4.5算法

**信息增益率** = 信息增益 ÷ 特征固有信息

会考虑特征本身的信息量



决策树剪枝

决策树生成过程中，如果不断追求纯度，树会非常深，训练集准确率高，但测试集可能下降

- 预剪枝：构建就限制 xxx 
- 后剪枝：合并节点后，测试集效果不下降，就剪



CRAT算法



应用：

一直问问题，不管问题离散还是连续，都能处理

疾病诊断，信用评分







# 逻辑斯蒂回归

> 加权求和各种特征，得出属于某一类的概率

**二项逻辑斯蒂回归：**

用于处理二分类问题

输入：一些特征（年龄，性别，邮件中的关键词）

输出：属于某个分类的概率

结果：结果 > 0.5 是正类，使用 **sigmoid函数** 



core demo：得病风险 = 0.3*年龄 + 0.5*是否吸烟 + 0.2*是否肥胖

training：先假设各种参数（0.3，0.5，0.2）然后使用梯度下降一步一步调整权重



**多项逻辑斯蒂回归：**

评估多种可能性（猫/狗/兔子）

使用 **sigmoid函数** 



极大似然估计 = 调整参数，让模型更准确





# 最大熵模型

> 给一些if条件，如果满足则更可能发生某件事



举个例子（词性标注）

假设我们要给“北京”打词性标签：

- 条件1：当前词是“北京” → 特征函数 f1(x,y) = 1 如果 y=名词，否则 0
- 条件2：前一个词是“去” → 特征函数 f2(x,y) = 1 如果 y=名词，否则 0
- 条件3：词长=2 → 特征函数 f3(x,y) = 1 如果 y=名词，否则 0







# 支持向量机模型 SVM

> 找一条线，把两类点分开

线性可分：一条线/一个面 完美分开两类点

硬间隔最大化：不仅要把两类分开，还要尽量离两边都远

软间隔最大化：允许少数点被分错，但是整体间隔尽可能大

核技巧：用核函数把数据映射到高维空间，在高维空间做分割，在原空间判别



支持向量机 vs 感知机

共性：都是找一个线或者面分割两边

区别：支持向量机提出，分割尽可能完美，距离两边都要远



**应用：**

文本分类 / 垃圾邮件检测（线性 SVM 在高维稀疏文本上表现很好）

人脸识别、图像分类（小到中等数据集）

生物信息（基因/蛋白分类）

手写数字识别（如 MNIST 的基线）

异常检测（one-class SVM）

信用风险、医学判别等需要强鲁棒性的二分类任务



**实际应用场景：**

| 应用场景     | 传统最佳算法     | 深度学习最佳算法       | 实际最常用 & 实用 |
| ------------ | ---------------- | ---------------------- | ----------------- |
| 垃圾邮件检测 | 朴素贝叶斯 / SVM | BERT / Transformer     | 朴素贝叶斯 / BERT |
| 人脸识别     | SVM + PCA        | CNN (FaceNet, ArcFace) | CNN               |
| 图像分类     | SVM + HOG        | CNN / ViT              | CNN               |
| 手写数字识别 | SVM / KNN        | LeNet / CNN            | CNN               |
| 信用风险评估 | 逻辑回归         | 神经网络 / GBDT        | LR + GBDT         |
| 医学判别     | SVM / 随机森林   | CNN / Transformer      | RF/GBDT + CNN     |

















sigmoid

softmax

极大似然估计

梯度下降

















